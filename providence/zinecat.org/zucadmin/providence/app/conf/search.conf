#
# Search configuration
#

# number of seconds to keep cached browses around
# set to 0 to disable caching
cache_timeout = 0

# Path to search index configuration file â€“ the file that
# determines what data gets indexed for search
search_indexing_config = <ca_conf_dir>/search_indexing.conf

# Maximum number of records to buffer indexing for before
# writing out to search engine. Generally, larger numbers provide better
# performance up to a point, at the expense of memory footprint. Larger 
# values may cause issues with some engines. If you are experiencing errors while 
# indexing, excessive memory use or lost indexing trying reducing the value of 
# this setting and then reindex.
#
max_indexing_buffer_size = 50000

# -------------------
# SqlSearch plugin configuration
# -------------------

# Set to 0 if you don't want search input stemmed (ie. suffixes removed) prior to search
# The plugin uses the English Snoball stemmer (http://snowball.tartarus.org/) and can give
# poor results with non-English content. If you are cataloguing non-English material you
# will probably want to turn this off.
search_sql_search_do_stemming = 1


# -------------------
# ElasticSearch plugin configuration
# -------------------

# Enter the elastic search base url here (without any index names)
search_elasticsearch_base_url = http://localhost:9200/

# This is the name of the ElasticSearch index used by CollectiveAccess.
# You probably don't need to change this unless you're using a single 
# ElasticSearch setup for multiple CollectiveAccess instances and/or
# other applications.
search_elasticsearch_index_name = collectiveaccess

# Indexing buffer size setting specifically for ElasticSearch
elasticsearch_indexing_buffer_size = 250

# -------------------
# General search and indexing behavior (all plugins)
# -------------------
# Suffixes to add to searches if they conform to a listed regular expression
search_suffixes = {
#	[\d]+\.[0-9A-Za-z\.]* = *
}

# Regex character class used when indexing; values matched will be used as token delimiters
# (in other words, the search expression will be broken into words wherever the matched characters are)
indexing_tokenizer_regex = ^\pL\pN\pNd/_#\@\&\-

# Regex character class used when searching; values matched will be used as token delimiters
# (this is the same thing as indexing_tokenizer_regex except that it's used when searching rather than indexing)
search_tokenizer_regex = ^\pL\pN\pNd/_#\@\&\-

# List of regular expressions that if matched cause search input to be treated "as-is" - searched without processing
# This is useful for preventing tokenization of accession numbers and other values that rely upon punctuation being
# kept intact.
asis_regexes = [
	"^[\d]+[\.\-][A-Za-z0-9\.\-]+$",
	"[^\.]+\.[A-Za-z0-9]{3}$"
]